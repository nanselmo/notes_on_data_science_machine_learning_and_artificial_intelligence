<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Data Science for Political and Social Phenomena">
    <meta name="author" content="Chris Albon">
    <link rel="icon" href="../favicon.ico">

    <title>Naive Bayes Classifier From Scratch - Machine Learning</title>

    <!-- JQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>
        window.jQuery || document.write('<script src="../theme/js/jquery.min.js"><\/script>')
    </script>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="../theme/css/bootstrap.css" />
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link rel="stylesheet" type="text/css" href="../theme/css/ie10-viewport-bug-workaround.css" />
    <!-- Custom styles for this template -->
    <link rel="stylesheet" type="text/css" href="../theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="../theme/css/notebooks.css" />
    <link href='https://fonts.googleapis.com/css?family=PT+Serif:400,700|Roboto:400,500,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
     <link href="http://chrisalbon.com/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Chris Albon - Data Science, Machine Learning, and Artificial Intelligence Full RSS Feed" />         <link href="http://chrisalbon.com/feeds/machine-learning.rss.xml" type="application/rss+xml" rel="alternate" title="Chris Albon - Data Science, Machine Learning, and Artificial Intelligence Categories RSS Feed" />    

    <meta name="tags" content="Basics" />


</head>

<body>

    <div class="navbar navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="..">Chris Albon</a>
            </div>
            <div class="navbar-collapse collapse" id="searchbar">

                <ul class="nav navbar-nav navbar-right">
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">About<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="../pages/about.html">About Chris</a></li>
                            <li><a href="https://github.com/chrisalbon">GitHub</a></li>
                            <li><a href="https://twitter.com/chrisalbon">Twitter</a></li>
                            <li><a href="https://www.linkedin.com/in/chrisralbon">LinkedIn</a></li>
                            <li><a href="https://pinboard.in/u:chrisalbon">Pinboard</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Data Science<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="..#Blog">Blog</a></li>
                            <li><a href="..#Python">Python</a></li>
                            <li><a href="..#R_Stats">R Stats</a></li>
                            <li><a href="..#Regex">Regex</a></li>
                            <li><a href="..#Bash">Bash</a></li>
                            <li><a href="..#Project_Juypter">Project Juypter</a></li>
                            <li><a href="..#SQL">SQL</a></li>
                            <li><a href="..#Mathematics">Mathematics</a></li>
                            <li><a href="..#Javascript">Javascript</a></li>
                            <li><a href="..#Probability">Probability</a></li>
                            <li><a href="..#Frequentist_Statistics">Frequentist Statistics</a></li>
                            <li><a href="..#Bayesian_Statistics">Bayesian Statistics</a></li>
                            <li><a href="..#Machine_Learning">Machine Learning</a></li>
                            <li><a href="..#GitHub">GitHub</a></li>
                            <li><a href="..#Scala">Scala</a></li>
                            <li><a href="..#Spark">Spark</a></li>
                            <li><a href="..#Amazon_Web_Services">Amazon Web Services</a></li>
                            <li><a href="..#Kaggle">Kaggle</a></li>
                            <li><a href="..#Projects">Projects</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Projects<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="http://newknowledge.io">New Knowledge</a></li>
                            <li><a href="http://partiallyderivative.com">Partially Derivative</a></li>
                            <li><a href="https://github.com/chrisalbon/notes_on_data_science_machine_learning_and_artificial_intelligence">Notes on DS, ML, and AI</a></li>
                        </ul>
                    </li>

                    <li class="dropdown">
                        <a href="../feeds/all.rss.xml">Feed</a>
                    </li>


                </ul>

                <form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);">
                    <div class="form-group" style="display:inline;">
                        <div class="input-group" style="display:table;">
                            <span class="input-group-addon" style="width:1%;"><span class="glyphicon glyphicon-search"></span></span>
                            <input class="form-control search-query" name="q" id="tipue_search_input" placeholder="e.g. scikit KNN, pandas merge" required autocomplete="off" type="text">
                        </div>
                    </div>
                </form>

            </div>
            <!--/.nav-collapse -->
        </div>
    </div>



    <!-- end of header section -->
    <div class="container">
<!-- <div class="alert alert-warning" role="alert">
    Did you find this page useful? Please do me a quick favor and <a href="#" class="alert-link">endorse me for data science on LinkedIn</a>.
</div> -->
<section id="content" class="body">
    <header>
    <h1>
      Naive Bayes Classifier From Scratch
    </h1>
<ol class="breadcrumb">
    <li>
        <time class="published" datetime="2016-12-12T12:00:00-08:00">
            12 December 2016
        </time>
    </li>
    <li>Machine Learning</li>
    <li>Basics</li>
</ol>
</header>
<div class='article_content'>
<p>Naive bayes is simple classifier known for doing well when only a small number of observations is available. In this tutorial we will create a gaussian naive bayes classifier from scratch and use it to predict the class of a previously unseen data point. This tutorial is based on an example on Wikipedia's <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive bayes classifier page</a>, I have implemented it in Python and tweaked some notation to improve explanation.</p>
<h2>Preliminaries</h2>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>


<h2>Create Data</h2>
<p>Our dataset is contains data on eight individuals. We will use the dataset to construct a classifier that takes in the height, weight, and foot size of an individual and outputs a prediction for their gender.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Create an empty dataframe</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="c1"># Create our target variable</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span><span class="s1">&#39;male&#39;</span><span class="p">,</span><span class="s1">&#39;male&#39;</span><span class="p">,</span><span class="s1">&#39;male&#39;</span><span class="p">,</span><span class="s1">&#39;female&#39;</span><span class="p">,</span><span class="s1">&#39;female&#39;</span><span class="p">,</span><span class="s1">&#39;female&#39;</span><span class="p">,</span><span class="s1">&#39;female&#39;</span><span class="p">]</span>

<span class="c1"># Create our feature variables</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mf">5.92</span><span class="p">,</span><span class="mf">5.58</span><span class="p">,</span><span class="mf">5.92</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">5.5</span><span class="p">,</span><span class="mf">5.42</span><span class="p">,</span><span class="mf">5.75</span><span class="p">]</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span><span class="mi">190</span><span class="p">,</span><span class="mi">170</span><span class="p">,</span><span class="mi">165</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">130</span><span class="p">,</span><span class="mi">150</span><span class="p">]</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">]</span>

<span class="c1"># View the data</span>
<span class="n">data</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Foot_Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>male</td>
      <td>6.00</td>
      <td>180</td>
      <td>12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>male</td>
      <td>5.92</td>
      <td>190</td>
      <td>11</td>
    </tr>
    <tr>
      <th>2</th>
      <td>male</td>
      <td>5.58</td>
      <td>170</td>
      <td>12</td>
    </tr>
    <tr>
      <th>3</th>
      <td>male</td>
      <td>5.92</td>
      <td>165</td>
      <td>10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>female</td>
      <td>5.00</td>
      <td>100</td>
      <td>6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>female</td>
      <td>5.50</td>
      <td>150</td>
      <td>8</td>
    </tr>
    <tr>
      <th>6</th>
      <td>female</td>
      <td>5.42</td>
      <td>130</td>
      <td>7</td>
    </tr>
    <tr>
      <th>7</th>
      <td>female</td>
      <td>5.75</td>
      <td>150</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>

<p>The dataset above is used to construct our classifier. Below we will create a new person for whom we know their feature values but not their gender. Our goal is to predict their gender.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Create an empty dataframe</span>
<span class="n">person</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="c1"># Create some feature values for this single row</span>
<span class="n">person</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">]</span>
<span class="n">person</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">130</span><span class="p">]</span>
<span class="n">person</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span>

<span class="c1"># View the data</span>
<span class="n">person</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Height</th>
      <th>Weight</th>
      <th>Foot_Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>130</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>

<h2>Bayes Theorem</h2>
<p>Bayes theorem is a famous equation that allows us to make predictions based on data. Here is the classic version of the Bayes theorem:</p>
<div class="math">$$\displaystyle P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}$$</div>
<p>This might be too abstract, so let us replace some of the variables to make it more concrete. In a bayes classifier, we are interested in finding out the class (e.g. male or female, spam or ham) of an observation <em>given</em> the data:</p>
<div class="math">$$p(\text{class} \mid \mathbf {\text{data}} )={\frac {p(\mathbf {\text{data}} \mid \text{class}) * p(\text{class})}{p(\mathbf {\text{data}} )}}$$</div>
<p>where:</p>
<ul>
<li><span class="math">\(\text{class}\)</span> is a particular class (e.g. male)</li>
<li><span class="math">\(\mathbf {\text{data}}\)</span> is an observation's data</li>
<li><span class="math">\(p(\text{class} \mid \mathbf {\text{data}} )\)</span> is called the posterior</li>
<li><span class="math">\(p(\text{class})\)</span> is called the likelihood</li>
<li><span class="math">\(p(\text{class})\)</span> is called the prior</li>
<li><span class="math">\(p(\mathbf {\text{data}} )\)</span> is called the marginal probability</li>
</ul>
<p>In a bayes classifier, we calculate the posterior (technically we only calculate the numerator of the posterior, but ignore that for now) for every class for each observation. Then, classify the observation based on the class with the largest posterior value. In our example, we have one observation to predict and two possible classes (e.g. male and female), therefore we will calculate two posteriors: one for male and one for female.</p>
<div class="math">$$p(\text{person is male} \mid \mathbf {\text{person's data}} )={\frac {p(\mathbf {\text{person's data}} \mid \text{person is male}) * p(\text{person is male})}{p(\mathbf {\text{person's data}} )}}$$</div>
<div class="math">$$p(\text{person is female} \mid \mathbf {\text{person's data}} )={\frac {p(\mathbf {\text{person's data}} \mid \text{person is female}) * p(\text{person is female})}{p(\mathbf {\text{person's data}} )}}$$</div>
<h2>Gaussian Naive Bayes Classifier</h2>
<p>A gaussian naive bayes is probably the most popular type of bayes classifier. To explain what the name means, let us look at what the bayes equations looks like when we apply our two classes (male and female) and three feature variables (height, weight, and footsize):</p>
<div class="math">$${\displaystyle {\text{posterior (male)}}={\frac {P({\text{male}})\,p({\text{height}}\mid{\text{male}})\,p({\text{weight}}\mid{\text{male}})\,p({\text{foot size}}\mid{\text{male}})}{\text{marginal probability}}}}$$</div>
<div class="math">$${\displaystyle {\text{posterior (female)}}={\frac {P({\text{female}})\,p({\text{height}}\mid{\text{female}})\,p({\text{weight}}\mid{\text{female}})\,p({\text{foot size}}\mid{\text{female}})}{\text{marginal probability}}}}$$</div>
<p>Now let us unpack the top equation a bit:</p>
<ul>
<li><span class="math">\(P({\text{male}})\)</span> is the prior probabilities. It is, as you can see, simply the probability an observation is male. This is just the number of males in the dataset divided by the total number of people in the dataset.</li>
<li><span class="math">\(p({\text{height}}\mid{\text{female}})\,p({\text{weight}}\mid{\text{female}})\,p({\text{foot size}}\mid{\text{female}})\)</span> is the likelihood. Notice that we have unpacked <span class="math">\(\mathbf {\text{person's data}}\)</span> so it is now every feature in the dataset. The "gaussian" and "naive" come from two assumptions present in this likelihood:<ol>
<li>If you look each term in the likelihood you will notice that we assume each feature is uncorrelated from each other. That is, foot size is independent of weight or height etc.. This is obviously not true, and is a "naive" assumption - hence the name "naive bayes."</li>
<li>Second, we assume have that the value of the features (e.g. the height of women, the weight of women) are normally (gaussian) distributed. This means that <span class="math">\(p(\text{height}\mid\text{female})\)</span> is calculated by inputing the required parameters into the probability density function of the normal distribution:</li>
</ol>
</li>
</ul>
<div class="math">$$
p(\text{height}\mid\text{female})=\frac{1}{\sqrt{2\pi\text{variance of female height in the data}}}\,e^{ -\frac{(\text{observation's height}-\text{average height of females in the data})^2}{2\text{variance of female height in the data}} }
$$</div>
<ul>
<li><span class="math">\(\text{marginal probability}\)</span> is probably one of the most confusing parts of bayesian approaches. In toy examples (including ours) it is completely possible to calculate the marginal probability. However, in many real-world cases, it is either extremely difficult or impossible to find the value of the marginal probability (explaining why is beyond the scope of this tutorial). This is not as much of a problem for our classifier as you might think. Why? Because we don't care what the true posterior value is, we only care which class has a the highest posterior value. And because the marginal probability is the same for all classes 1) we can ignore the denominator, 2) calculate only the posterior's numerator for each class, and 3) pick the largest numerator. That is, we can ignore the posterior's denominator and make a prediction solely on the relative values of the posterior's numerator.</li>
</ul>
<p>Okay! Theory over. Now let us start calculating all the different parts of the bayes equations.</p>
<h2>Calculate Priors</h2>
<p>Priors can be either constants or probability distributions. In our example is the simply the probability of being a gender. Calculating this is simple:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Number of males</span>
<span class="n">n_male</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">][</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1"># Number of males</span>
<span class="n">n_female</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">][</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1"># Total rows</span>
<span class="n">total_ppl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># Number of males divided by the total rows</span>
<span class="n">P_male</span> <span class="o">=</span> <span class="n">n_male</span><span class="o">/</span><span class="n">total_ppl</span>

<span class="c1"># Number of females divided by the total rows</span>
<span class="n">P_female</span> <span class="o">=</span> <span class="n">n_female</span><span class="o">/</span><span class="n">total_ppl</span>
</pre></div>


<h2>Calculate Likelihood</h2>
<p>Remember that each term (e.g. <span class="math">\(p(\text{height}\mid\text{female})\)</span>) in our likelihood is assumed to be a normal pdf. For example:</p>
<div class="math">$$
p(\text{height}\mid\text{female})=\frac{1}{\sqrt{2\pi\text{variance of female height in the data}}}\,e^{ -\frac{(\text{observation's height}-\text{average height of females in the data})^2}{2\text{variance of female height in the data}} }
$$</div>
<p>This means that for each class (e.g. female) and feature (e.g. height) combination we need to calculate the variance and mean value from the data. Pandas makes this easy:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Group the data by gender and calculate the means of each feature</span>
<span class="n">data_means</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Gender&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># View the values</span>
<span class="n">data_means</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Height</th>
      <th>Weight</th>
      <th>Foot_Size</th>
    </tr>
    <tr>
      <th>Gender</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>5.4175</td>
      <td>132.50</td>
      <td>7.50</td>
    </tr>
    <tr>
      <th>male</th>
      <td>5.8550</td>
      <td>176.25</td>
      <td>11.25</td>
    </tr>
  </tbody>
</table>
</div>

<div class="codehilite"><pre><span></span><span class="c1"># Group the data by gender and calculate the variance of each feature</span>
<span class="n">data_variance</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Gender&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>

<span class="c1"># View the values</span>
<span class="n">data_variance</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Height</th>
      <th>Weight</th>
      <th>Foot_Size</th>
    </tr>
    <tr>
      <th>Gender</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>0.097225</td>
      <td>558.333333</td>
      <td>1.666667</td>
    </tr>
    <tr>
      <th>male</th>
      <td>0.035033</td>
      <td>122.916667</td>
      <td>0.916667</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now we can create all the variables we need. The code below might look complex but all we are doing is creating a variable out of each cell in both of the tables above.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Means for male</span>
<span class="n">male_height_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">male_weight_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">male_footsize_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Variance for male</span>
<span class="n">male_height_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">male_weight_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">male_footsize_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Means for female</span>
<span class="n">female_height_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">female_weight_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">female_footsize_mean</span> <span class="o">=</span> <span class="n">data_means</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Variance for female</span>
<span class="n">female_height_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">female_weight_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">female_footsize_variance</span> <span class="o">=</span> <span class="n">data_variance</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="n">data_variance</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>Finally, we need to create a function to calculate the probability density of each of the terms of the likelihood (e.g. <span class="math">\(p(\text{height}\mid\text{female})\)</span>).</p>
<div class="codehilite"><pre><span></span><span class="c1"># Create a function that calculates p(x | y):</span>
<span class="k">def</span> <span class="nf">p_x_given_y</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_y</span><span class="p">,</span> <span class="n">variance_y</span><span class="p">):</span>

    <span class="c1"># Input the arguments into a probability density function</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">variance_y</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mean_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">variance_y</span><span class="p">))</span>

    <span class="c1"># return p</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>


<h2>Apply Bayes Classifier To New Data Point</h2>
<p>Alright! Our bayes classifier is ready. Remember that since we can ignore the marginal probability (the demoninator), what we are actually calculating is this:</p>
<div class="math">$${\displaystyle {\text{numerator of the posterior}}={P({\text{female}})\,p({\text{height}}\mid{\text{female}})\,p({\text{weight}}\mid{\text{female}})\,p({\text{foot size}}\mid{\text{female}})}{}}$$</div>
<p>To do this, we just need to plug in the values of the unclassified person (height = 6), the variables of the dataset (e.g. mean of female height), and the function (<code>p_x_given_y</code>) we made above:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Numerator of the posterior if the unclassified observation is a male</span>
<span class="n">P_male</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">male_height_mean</span><span class="p">,</span> <span class="n">male_height_variance</span><span class="p">)</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">male_weight_mean</span><span class="p">,</span> <span class="n">male_weight_variance</span><span class="p">)</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">male_footsize_mean</span><span class="p">,</span> <span class="n">male_footsize_variance</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>6.1970718438780782e-09
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># Numerator of the posterior if the unclassified observation is a female</span>
<span class="n">P_female</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Height&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">female_height_mean</span><span class="p">,</span> <span class="n">female_height_variance</span><span class="p">)</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">female_weight_mean</span><span class="p">,</span> <span class="n">female_weight_variance</span><span class="p">)</span> <span class="o">*</span> \
<span class="n">p_x_given_y</span><span class="p">(</span><span class="n">person</span><span class="p">[</span><span class="s1">&#39;Foot_Size&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">female_footsize_mean</span><span class="p">,</span> <span class="n">female_footsize_variance</span><span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span>0.00053779091836300176
</pre></div>


<p>Because the numerator of the posterior for male is greater than female, then we predict that the person is male.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
    <aside>
    <div class="bug-reporting__panel">
        <h3>Find an error or bug? Have a suggestion?</h3>
        <p>Everything on this site is avaliable on GitHub. Head on over and <a href='https://github.com/chrisalbon/notes_on_data_science_machine_learning_and_artificial_intelligence/issues/new'>submit an issue.</a> You can also message me directly on <a href='https://twitter.com/chrisalbon'>Twitter</a>.</p>
    </div>
    </aside>
</section>

    </div>
    <!-- start of footer section -->
    <footer class="footer">
        <div class="container">
            <p class="text-muted">
                <center>This project contains 409 pages and is available on <a href="https://github.com/chrisalbon/notes_on_data_science_machine_learning_and_artificial_intelligence">GitHub</a>.
                <br/>
                Copyright &copy; Chris Albon,
                    <time datetime="2016">2016</time>.
                </center>
            </p>
        </div>
    </footer>

    <!-- This jQuery line finds any span that contains code highlighting classes and then selects the parent <pre> tag and adds a border. This is done as a workaround to visually distinguish the code inputs and outputs -->
    <script>
        $( ".hll, .n, .c, .err, .k, .o, .cm, .cp, .c1, .cs, .gd, .ge, .gr, .gh, .gi, .go, .gp, .gs, .gu, .gt, .kc, .kd, .kn, .kp, .kr, .kt, .m, .s, .na, .nb, .nc, .no, .nd, .ni, .ne, .nf, .nl, .nn, .nt, .nv, .ow, .w, .mf, .mh, .mi, .mo, .sb, .sc, .sd, .s2, .se, .sh, .si, .sx, .sr, .s1, .ss, .bp, .vc, .vg, .vi, .il" ).parent( "pre" ).css( "border", "1px solid #DEDEDE" );
    </script>

    <!-- Load Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-66582-32', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End of Google Analytics -->

    <!-- Bootstrap core JavaScript
      ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../theme/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../theme/js/ie10-viewport-bug-workaround.js"></script>
</body>

</html>