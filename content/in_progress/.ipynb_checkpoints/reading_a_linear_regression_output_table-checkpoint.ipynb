{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Reading A Linear Regression Output Table  \n",
    "Slug: reading_a_linear_regression_output_table  \n",
    "Summary: Reading A Linear Regression Output Table  \n",
    "Date: 2016-05-01 12:00  \n",
    "Category: Frequentist Statistics \n",
    "Tags: Basics  \n",
    "Authors: Chris Albon  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [An Introduction to Statistical Learning](http://amzn.to/2jsf0Iy), [ISL-python repo](http://nbviewer.jupyter.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%203.ipynb).\n",
    "\n",
    "Data source: [ISL's webpage](http://www-bcf.usc.edu/~gareth/ISL/data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes many of the statistics you will see in a typical regression output table. While this particular regression is run using the statsmodels package for Python, a similar chart will be seen in SPSS, STATA, R, or other statistical software.\n",
    "\n",
    "The data for this tutorial comes from [An Introduction to Statistical Learning](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370) and the regression run is the exact regression described in the book. For this reason, if you want a more indepth description of anything you see here, check out the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "advertising = pd.read_csv('../data/isl/Advertising.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a simple ordinary least squares regression that trains a model to predict a company's sales (in dollars) compared to the amount of dollars the company spends on TV ads, radio ads, or newspaper ads. Specifically, the model is:\n",
    "\n",
    "$$\\hat{y}_{sales} = \\beta_{0}+\\beta_{1}x_{Television}+\\beta_{2}x_{Radio}+\\beta_{3}x_{Newspaper}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "est = smf.ols('Sales ~ TV + Radio + Newspaper', advertising).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two table below should be the output tables you are used to seeing. Often they are displayed together, however for the sake of organization I explain the elements of each one seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   570.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 19 Aug 2016</td> <th>  Prob (F-statistic):</th> <td>1.58e-96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:46:38</td>     <th>  Log-Likelihood:    </th> <td> -386.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   780.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   196</td>      <th>  BIC:               </th> <td>   793.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first output table\n",
    "est.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. Observations\n",
    "\n",
    "The number of observations used in the analysis. In this case, we trained our model using 200 observations.\n",
    "\n",
    "### Df Residuals\n",
    "\n",
    "The number of degrees of freedom remaining. That is, the total degrees of freedom minus the model degrees of freedom).\n",
    "\n",
    "### Df Model\n",
    "\n",
    "The number of degrees of freedom used to train the model. In this case we trained our model to find four coefficients ($\\beta_{0}$+$\\beta_{1}$,$\\beta_{2}$, and $\\beta_{3}$) and thus used the number of coefficients minus one degrees of freedom.\n",
    "\n",
    "### R-squared\n",
    "\n",
    "The proportion of the variance in the dependent variable (in this case `sales`) explained by the independent variables (`TV`,`Radio`, and `Newspaper`). R-squared is commonly used to judge how well the model fits the data. R-squared rages between 0 and 1 and is independent of the unit scale (in our model, dollars spent) of the data itself.\n",
    "\n",
    "R-squared is calculated:\n",
    "\n",
    "$$ R^{2} = 1 - \\frac{RSS}{TSS} $$\n",
    "\n",
    "where $TSS$ is the total sum of squares, a measure of the sum of the squared difference between each value of the dependent variable and the dependent variable's mean value:\n",
    "\n",
    "$$\\sum _{{i=1}}^{{n}}\\left(y_{{i}}-{\\bar  {y}}\\right)^{2}$$ \n",
    "\n",
    "and where $RSS$ is the residual sum of squares, a measure of the model's accuracy. Specifically it is the sum of the squared difference between each of the trained model's predicted y values and the actual y values:\n",
    "\n",
    "$$\\sum_{i=1}^{n}(y_{i}-f(x_{i}))^{2}$$\n",
    "\n",
    "### Adj. R-squared\n",
    "\n",
    "One problem with R-squared is that as you increase the number of predictors in a model, the variance explained by that model will increase, even if those predictors are not related to the response variable. This can become a problem if you are comparing the R-squared of the model with two predictors with the R-squared of a model with 200 predictors. In this case, the second model might have a greater R-squared simply because it contains more predictors. Adjusted R-squared compensates for this:\n",
    "\n",
    "$$R^{2}_{Adjusted} = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "\n",
    "where $n$ is the number of observations, and $d$ is the number of predictors (independent variables).\n",
    "\n",
    "Because the number of predictors, $d$ is included in the calculation, adjusted R-squared statistic in penalized for each additional predictor. The idea is that each new predictor added has to be useful enough to the model that it is worth the penalty paid for its inclusion.\n",
    "\n",
    "### F-statistic\n",
    "\n",
    "The F-statistic tests whether all the coefficients are equal to zero:\n",
    "\n",
    "$$ \\beta_{1} =\\beta_{2}=\\cdots \\beta_{n}=0 $$\n",
    "\n",
    "The F-statistic is calculated:\n",
    "\n",
    "$$F=\\frac{(TSS-RSS)/p}{RSS/(n-d-1)}$$\n",
    "\n",
    "where $p$ is the number of predictors. The statistic works because based on the linear model assumptions we assume the expectation of the demoninator is also $\\sigma^{2}$ and if all the coefficients collectively equal zero, then the expectation of the numerator is $\\sigma^{2}$. In that case the F-statistic will be close to 1.\n",
    "\n",
    "The reason why might look at the F-statistic first, instead of the individual coefficient p-values, is because if you have a large number of predictors unrelated to the response variable, just by chance some of them will be statistically significant. The F-statistic does not face this problem because $p$ in the denominator penalizes the statistic for each additional predictor.\n",
    "\n",
    "### Prob (F-statistic)\n",
    "\n",
    "The p-value of the F-statistic. If it is not significant, then we cannot reject the null hypothesis:\n",
    "\n",
    "$$ H_{0}:\\beta_{1} =\\beta_{2}=\\cdots \\beta_{n}=0 $$\n",
    "\n",
    "### AIC\n",
    "\n",
    "Akaike information criterion (AIC) is another measure of the quality of the model, like R-squared. In general models with lower AIC are preferable to models with higher AIC.\n",
    "\n",
    "$$AIC=\\frac{1}{n\\sigma^{2}}(RSS+2d\\sigma^{2})$$\n",
    "\n",
    "where $2d\\sigma^{2}$ is essentially a penalty placed upon the score to account for the fact that the errors in training models well tend to be naturally lower than errors in test data.\n",
    "\n",
    "where $n$ is the number of observations and $d$ is the number of predictors. \n",
    "\n",
    "### BIC\n",
    "\n",
    "BIC is like AIC, taking a small value for a model with a low test error, the $2d\\sigma^{2}$ penalty term replaces the $2$ used in AIC with $log(n)$.\n",
    "\n",
    "$$ BIC = \\frac{1}{n}(RSS+log(n)d\\sigma^{2}) $$\n",
    "\n",
    "Since $log(n)$ will be greater than 2 for models with more than seven observations, BIC has a greater penalty for models with many variables than AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.9389</td> <td>    0.312</td> <td>    9.422</td> <td> 0.000</td> <td>    2.324     3.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.809</td> <td> 0.000</td> <td>    0.043     0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radio</th>     <td>    0.1885</td> <td>    0.009</td> <td>   21.893</td> <td> 0.000</td> <td>    0.172     0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Newspaper</th> <td>   -0.0010</td> <td>    0.006</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.013     0.011</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the second output table\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coef\n",
    "\n",
    "The effect size. Specifically, in the case of linear regression -- the change in $y$ per one unit change of $X$.\n",
    "\n",
    "### std err\n",
    "\n",
    "The standard deviation of the estimate of a coefficient. It is a measure of how well the model estimates the unknown true coefficient value.\n",
    "\n",
    "### t\n",
    "\n",
    "Calculated as the coefficient estimate divided by its standard error, the t-statistic tests whether the coefficient is different from zero.\n",
    "\n",
    "### P>|t|\n",
    "\n",
    "The p-value of the t-statistic.\n",
    "\n",
    "### [95.0% Conf. Int.]\n",
    "\n",
    "The 95% confidence interval. There is a 95% probability the true coefficient value is between the two values. If the interval includes zero, then the we cannot reject the null hypthosis that a coefficient has no effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
